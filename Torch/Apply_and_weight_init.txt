
use layer.apply(function) to apply init_weights 
def init_weights(m):
    classname = m.__class__.__name__
    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:
        nn.init.kaiming_uniform_(m.weight)
        nn.init.zeros_(m.bias)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight, 1.0, 0.02)
        nn.init.zeros_(m.bias)
    elif classname.find('Linear') != -1:
        nn.init.xavier_normal_(m.weight)
        nn.init.zeros_(m.bias)

nn.init has many forms, use kaiming_uniform as often as possible, 

notice that classname here is a string and more info can be extracted from it
the apply() uses a recursive call, i.e it applyies on submodusle and subchildren of the modle as well clearly on the basis of the if condition it applies it further 
(even for nn.sequential it applies on ever single submodule) 

Another way to do it would be 
	module.weight.data.fill_(value)
	torch.nn.init.xavier_uniform_(value)
	conv1.weight.bias.data.fill_(0.0)


instead of using classname you can use 
	if(isinstance(m, nn.Linear)):
		// continue doing this
